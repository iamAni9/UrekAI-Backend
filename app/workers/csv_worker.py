from app.config.logger import get_logger
from app.config.constants import MAX_UPLOAD_RETRIES, SAMPLE_ROW_LIMIT
from app.utils.db_utils import remove_analysis, delete_temp_table, create_table_from_schema, update_upload_progress_in_queue
from app.utils.schema_generation import generate_table_schema
# from app.utils.cloud_file_bucket import download_and_save_file
from app.helper.csv_worker_helper import get_sample_rows, add_data_into_table_from_csv
import os, math, asyncio
# from app.config.settings import settings

logger = get_logger("CSV Worker")

async def fetch_next_csv_job(conn):
    try:
        async with conn.transaction():
            row = await conn.fetchrow("""
                UPDATE csv_queue
                SET status = 'processing'
                WHERE id = (
                    SELECT id FROM csv_queue
                    WHERE status = 'pending'
                    ORDER BY created_at
                    FOR UPDATE SKIP LOCKED
                    LIMIT 1
                )
                RETURNING *
            """)
            return row
        logger.info("Checking job successful.")
    except Exception as e:
        logger.error(f"Error while updating the csv queue, {e}")
        raise

async def csv_processing(conn):
    job = await fetch_next_csv_job(conn)
    if job:
        await handle_job(dict(job), conn)

async def handle_job(job, conn):
    try:
        file_path = job["file_path"]
        # if settings.ENV != 'development':
        #     file_path = await download_and_save_file(file_path)
        table_name = job["table_name"]
        userid = job["user_id"]
        email = job["email"]
        upload_id = job["upload_id"]
        original_file_name = job["original_file_name"]
        
        for attempt in range(1, MAX_UPLOAD_RETRIES + 1):
            table_created = False
            analysis_done = False
            try:
                logger.info(f"Starting CSV processing attempt {attempt}/{MAX_UPLOAD_RETRIES} || File Path: {file_path}, Upload Id: {upload_id}")

                # Step 1: Getting sample data from uploaded file
                sample_rows = await get_sample_rows(file_path, SAMPLE_ROW_LIMIT)
                logger.info(f"Sample rows extracted {sample_rows['row01']}")
                await update_upload_progress_in_queue(conn, 'csv_queue', logger, upload_id, 10)
                
                # Step 2: Generating schema using LLM
                table_schema = await generate_table_schema(conn, userid, table_name, original_file_name, sample_rows, logger)
                if not table_schema:
                    raise Exception("Schema generation returned None")
                
                await update_upload_progress_in_queue(conn, 'csv_queue', logger, upload_id, 30)
                analysis_done = True
                
                schema = table_schema["schema"]
                contain_columns = table_schema["contain_columns"]
                # logger.info(f"Schema: {schema}")
                # logger.info(f"Contain Column: {contain_columns}")
                
                # Step 3: Creating DB table using schema generated by LLM                
                await create_table_from_schema(conn, table_name, schema, logger)
                table_created = True
                await update_upload_progress_in_queue(conn, 'csv_queue', logger, upload_id, 70)

                # Step 4: Inserting full CSV into DB table
                await add_data_into_table_from_csv(conn, file_path, table_name, schema, contain_columns["contain_column"])
                logger.info(f"CSV processing completed successfully for upload {upload_id}")
                await update_upload_progress_in_queue(conn, 'csv_queue', logger, upload_id, 100, "completed")
                
                return  

            except Exception as e:
                logger.error(f"CSV processing attempt {attempt} failed for upload_id {upload_id}, {e}")
                
                if analysis_done:
                    await remove_analysis(conn, userid, table_name, logger)
                
                if table_created:
                    await delete_temp_table(conn, table_name, logger)
                
            if attempt == MAX_UPLOAD_RETRIES:
                logger.error(f"All {MAX_UPLOAD_RETRIES} attempts failed for upload {upload_id}")
                await update_upload_progress_in_queue(conn, 'csv_queue', logger, upload_id, 100, "failed")
                raise

            # Retry delay (exponential backoff)
            wait_time = math.pow(2, attempt)
            logger.info(f"Retrying after {wait_time:.1f} seconds")
            await asyncio.sleep(wait_time)

            # Cleanup if partially created
            if table_created:
                await delete_temp_table(conn, table_name)

        # Deleting file received
        os.remove(file_path)
        logger.info(f"Temporary CSV file deleted: {file_path}")
    except Exception as e:
        await update_upload_progress_in_queue(conn, 'csv_queue', logger, upload_id, 0, "failed")
        raise